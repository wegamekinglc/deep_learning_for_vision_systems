{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "In this notebook, you'll build a GAN using convolutional layers in the generator and discriminator. This is called a Deep Convolutional GAN, or DCGAN for short. The DCGAN architecture was first explored last year and has seen impressive results in generating new images, you can read the [original paper here](https://arxiv.org/pdf/1511.06434.pdf).\n",
    "\n",
    "You'll be training DCGAN on the [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) (SVHN) dataset. These are color images of house numbers collected from Google street view. SVHN images are in color and much more variable than MNIST. \n",
    "\n",
    "![SVHN Examples](assets/SVHN_examples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import glob\n",
    "\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "Here you can download the SVHN dataset. Run the cell above and it'll download to your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import h5py \n",
    "import keras\n",
    "\n",
    "\n",
    "def load_images(path):\n",
    "    train_images = sio.loadmat(path+'/train_32x32.mat')\n",
    "    test_images = sio.loadmat(path+'/test_32x32.mat')\n",
    "\n",
    "    return train_images, test_images\n",
    "\n",
    "def normalize_images(images):\n",
    "    imgs = images[\"X\"]\n",
    "    imgs = np.transpose(imgs, (3, 0, 1, 2))\n",
    "\n",
    "    labels = images[\"y\"]\n",
    "    # replace label \"10\" with label \"0\"\n",
    "    labels[labels == 10] = 0\n",
    "\n",
    "    # normalize images so pixel values are in range [0,1]\n",
    "    scalar = 1 / 255.\n",
    "    imgs = imgs * scalar\n",
    "\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "def save_data(images, labels, name):\n",
    "    with h5py.File(name+\".hdf5\", \"w\") as f:\n",
    "        f.create_dataset(\"X\", data=images, shape=images.shape, dtype='float32', compression=\"gzip\")\n",
    "        f.create_dataset(\"Y\", data=labels, shape=labels.shape, dtype='int32', compression=\"gzip\")\n",
    "        \n",
    "train_images, test_images = load_images(\"data/\")\n",
    "\n",
    "train_images_normalized, train_labels = normalize_images(train_images)\n",
    "save_data(train_images_normalized, train_labels, \"SVHN_train\")\n",
    "\n",
    "test_images_normalized, test_labels = normalize_images(test_images)\n",
    "save_data(test_images_normalized, test_labels, \"SVHN_test\")\n",
    "\n",
    "\n",
    "# load svhn data from the specified folder\n",
    "def load_svhn_data(path, val_size):\n",
    "    with h5py.File(path+'/SVHN_train.hdf5', 'r') as f:\n",
    "        shape = f[\"X\"].shape\n",
    "        x_train = f[\"X\"][:shape[0]-val_size]\n",
    "        y_train = f[\"Y\"][:shape[0]-val_size].flatten()\n",
    "        x_val = f[\"X\"][shape[0]-val_size:]\n",
    "        y_val = f[\"Y\"][shape[0] - val_size:].flatten()\n",
    "\n",
    "    with h5py.File(path+'/SVHN_test.hdf5', 'r') as f:\n",
    "        x_test = f[\"X\"][:]\n",
    "        y_test = f[\"Y\"][:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_svhn_data('training_dataset',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[7].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "Here you'll build the generator network. The input will be our noise vector `z` as before. Also as before, the output will be a $tanh$ output, but this time with size 32x32 which is the size of our SVHN images.\n",
    "\n",
    "What's new here is we'll use convolutional layers to create our new images. The first layer is a fully connected layer which is reshaped into a deep and narrow layer, something like 4x4x1024 as in the original DCGAN paper. Then we use batch normalization and a leaky ReLU activation. Next is a transposed convolution where typically you'd halve the depth and double the width and height of the previous layer. Again, we use batch normalization and leaky ReLU. For each of these layers, the general scheme is convolution > batch norm > leaky ReLU.\n",
    "\n",
    "You keep stacking layers up like this until you get the final transposed convolution layer with shape 32x32x3. Below is the archicture used in the original DCGAN paper:\n",
    "\n",
    "![DCGAN Generator](assets/dcgan.png)\n",
    "\n",
    "Note that the final layer here is 64x64x3, while for our SVHN dataset, we only want it to be 32x32x3.\n",
    "\n",
    "\n",
    "## Discriminator\n",
    "\n",
    "Here you'll build the discriminator. This is basically just a convolutional classifier like you've build before. The input to the discriminator are 32x32x3 tensors/images. You'll want a few convolutional layers, then a fully connected layer for the output. As before, we want a sigmoid output, and you'll need to return the logits as well. For the depths of the convolutional layers I suggest starting with 16, 32, 64 filters in the first layer, then double the depth as you add layers. Note that in the DCGAN paper, they did all the downsampling using only strided convolutional layers with no maxpool layers.\n",
    "\n",
    "You'll also want to use batch normalization with `tf.layers.batch_normalization` on each layer except the first convolutional and output layers. Again, each layer should look something like convolution > batch norm > leaky ReLU. \n",
    "\n",
    "Note: in this project, your batch normalization layers will always use batch statistics. (That is, always set `training` to `True`.) That's because we are only interested in using the discriminator to help train the generator. However, if you wanted to use the discriminator for inference later, then you would need to set the `training` parameter appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512 * 8 * 8, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((8, 8, 512)))\n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (x_val, y_val), (x_test, y_test) = load_svhn_data('training_dataset',0)\n",
    "        # Rescale 0 to 1\n",
    "        X_train = X_train.astype('float32')/255\n",
    "        # X_train = np.expand_dims(X_train, axis=3)\n",
    "        print(X_train[0].shape)\n",
    "    \n",
    "    \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, self.latent_dim))\n",
    "        generated_images = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "        fig, axis = plt.subplots(rows, columns)\n",
    "        \n",
    "        image_count = 0\n",
    "        for row in range(rows):\n",
    "            for column in range(columns):\n",
    "                # axis[row,column].imshow(generated_images[image_count, :,:,0])\n",
    "                axis[row,column].imshow(generated_images[image_count, :], cmap='spring')\n",
    "                axis[row,column].axis('off')\n",
    "                image_count += 1\n",
    "        fig.savefig(\"images2/svnh_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_111 (Conv2D)          (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_112 (Conv2D)          (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)   (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_113 (Conv2D)          (None, 5, 5, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)   (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_114 (Conv2D)          (None, 5, 5, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_115 (Conv2D)          (None, 5, 5, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 12801     \n",
      "=================================================================\n",
      "Total params: 1,585,217\n",
      "Trainable params: 1,583,297\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 32768)             3309568   \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_24 (UpSampling (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 16, 16, 128)       589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_25 (UpSampling (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 32, 32, 3)         867       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 4,030,723\n",
      "Trainable params: 4,030,147\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.517565, acc.: 29.69%] [G loss: 0.789728]\n",
      "1 [D loss: 0.234785, acc.: 89.06%] [G loss: 3.029856]\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN()\n",
    "dcgan.train(epochs=2, batch_size=32, save_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdeeplearning",
   "language": "python",
   "name": "tfdeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
