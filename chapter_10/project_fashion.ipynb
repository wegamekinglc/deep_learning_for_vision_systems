{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Visual Embedding Chapter\n",
    "## Fashion - select visually similar apparels \n",
    "### This notebook is for illustrating how to train a model using triplet-metric-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level summary of steps\n",
    "1. Download the dataset.\n",
    "2. Use the pre-processor script to generate .csv files containing list of images and their identities. These csv will be used for training and testing.\n",
    "3. Train the network. (Download the pre-trained tensorflow imagenet weights)\n",
    "4. Test the output network - both qualitatively and quantitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients:\n",
    "### Machine configuration\n",
    "* A machine with a GPU supporting CUDA 9.2+\n",
    "* Ubuntu 16.04 or later\n",
    "* Tensorflow 1.4+\n",
    "* Python 3\n",
    "An alternative to build above environment is to directly use a docker with relevant infomation. You can download from here - [TF Docker](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_19.07.html#rel_19.07).\n",
    "* For qualitative testing scripts - Opencv and Annoy (for nearest neighbors) - `pip install annoy` and `pip install opencv-python`\n",
    "\n",
    "#### In the following we used python `virtualenv` to create an environment with above pre-prequisite configuration.\n",
    "Please execute the following on a linux terminal.\n",
    "\n",
    "1. `python3 -m pip install --upgrade pip`\n",
    "2. `pip3 install virtualenv`\n",
    "3. `mkdir ~/venv`\n",
    "4. `virtualenv -p /usr/bin/python3.5 ~/venv/tf_1.9.0_cuda9.2`\n",
    "5. Install tensorflow - `pip install tensorflow_gpu==1.9.0`\n",
    "6. Optionally install visualization stuffs - `pip install annoy` and `pip install opencv-python`\n",
    "\n",
    "**Below we use Nvidia Cuda 9.2 on GTX 2080Ti.**\n",
    "\n",
    "### Steps to download above dataset\n",
    "1. Visit the link to download on item 2 [In-Shop Clothes Retrieval Benchmark](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) \n",
    "![title](viz/in-shop-retrieval.png)\n",
    "2. This is a google drive link and please go to folder \"In shop clothes retrieval benchmark\".\n",
    "3. Download the 3 folders -- `Anno/` , `Eval/` and `Img/`\n",
    "4. Unzip them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "In order to be able to use the training code, we will generate list of images (train, test, query) into a csv file - thereafter we will be able to feed in to the training/testing code seamlessly.\n",
    "\n",
    "*REQUIREMENT* The dataset (image_location) and (class_name) needs to be put in a csv format. A sample two row of a csv file would look like:\n",
    "\n",
    "`train_images, 12`\n",
    "\n",
    "`train_images, 33`\n",
    "\n",
    "So there will be a need of three output files, one for each of - `train_set`, `test_set` or `query_set`.\n",
    "\n",
    "(The folder `preprocessors/` hasn an utility function for obtaining the csv from the downloaded dataset. \n",
    "Fashion - `Fashion_convert2defense_triplet_format.py` . Please replace the variable `split_file` appropriately from the downloaded dataset.)\n",
    "\n",
    "**Alternatively** you could use the cell below to get the required csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET up variables for pre-processor\n",
    "# Variable split_file is obtained from downaloading the datasets\n",
    "split_file = \"/datasets/fashion/Eval/list_eval_partition.txt\" # this text file is inside the downloaded folder\n",
    "# following are the output CSV's we will need for train, test. \n",
    "output_train_csv = \"/datasets/fashion/in_shop_defense_triplet_loss_format_TRAIN.csv\"\n",
    "output_query_csv = \"/datasets/fashion/in_shop_defense_triplet_loss_format_QUERY.csv\"\n",
    "output_gallery_csv = \"/datasets/fashion/in_shop_defense_triplet_loss_format_GALLERY.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapper = {}\n",
    "id_counter = -1\n",
    "# This snippet uses the configuration input and output paths set above\n",
    "with open(split_file) as fp, open(output_train_csv, \"w\") as tr, open(output_gallery_csv, \"w\") as ga, open(output_query_csv, \"w\") as qu:\n",
    "    line = fp.readline()\n",
    "    cnt = 0\n",
    "    while line:\n",
    "        line = line.strip()  # remove leading and trailing whitespaces\n",
    "        cnt += 1\n",
    "        if cnt >= 3:\n",
    "            metadata = []\n",
    "            for tmp in line.split(\" \"):\n",
    "                if len(tmp) is not 0:\n",
    "                    metadata.append(tmp)\n",
    "            #print(\"metadata: {}\".format(metadata)) # if you want to display the inputs in terminal\n",
    "            _path = metadata[0]\n",
    "            _id = metadata[1]\n",
    "            _categ = metadata[2]\n",
    "            #print(\"_categ: {}\".format(_categ))\n",
    "            assert(_categ == \"train\" or _categ ==\n",
    "                   \"query\" or _categ == \"gallery\")\n",
    "            if _id not in id_mapper.keys():\n",
    "                id_counter += 1\n",
    "                id_mapper[_id] = id_counter\n",
    "            if _categ == \"train\":\n",
    "                tmp_str = str(id_counter) + \",\" + _path\n",
    "                tr.write(tmp_str)\n",
    "                tr.write(\"\\n\")\n",
    "            elif _categ == \"query\":\n",
    "                tmp_str = str(id_counter) + \",\" + _path\n",
    "                qu.write(tmp_str)\n",
    "                qu.write(\"\\n\")\n",
    "            elif _categ == \"gallery\":\n",
    "                tmp_str = str(id_counter) + \",\" + _path\n",
    "                ga.write(tmp_str)\n",
    "                ga.write(\"\\n\")\n",
    "            else:\n",
    "                print(\"Not possible to reach here!! \")\n",
    "        line = fp.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please check the folder for the output csv from the above pre-processor. There should be 3 output csvs as indicated (set) in the pre-processor above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the infrastructure code (training and testing)\n",
    "\n",
    "Location - [Github](https://github.com/VisualComputingInstitute/triplet-reid/tree/sampling)\n",
    "*Note* that we are cloning the `sampling` branch which has three sampling (mining variants):\n",
    "1. Batch All (BA)\n",
    "2. Batch Hard (BH)\n",
    "3. Batch Sample (BS)\n",
    "\n",
    "In order to freeze the repo status used for the book results, you can refer to the fork here - [Link](https://github.com/ratnesh1729/triplet-reid/tree/sampling)\n",
    "\n",
    "#### Steps to follow to procure the above codebase\n",
    "Please execute the following on Linux Terminal.\n",
    "* For the project, we will clone it inside `/code/`\n",
    "* `cd /code` \n",
    "* `git clone https://github.com/VisualComputingInstitute/triplet-reid`\n",
    "* `git checkout sampling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyper-parameters \n",
    "\n",
    "We have the following parameters need to be set for training:\n",
    "1. Network model: Choose a base architecture: Above code supplies options for Resnet-50, Resnet-101, MobileNet_v1. \n",
    "2. Pre-trained: Whether we need pre-trained model of above - Generally the answer is `yes`. \n",
    "3. Data augmentation: We could choose to randomly flip and crop images. \n",
    "4. Embedding dimension: Any feasible number. \n",
    "5. Batch Size: Parameterized by `P, K` , corresponding to `P` number of classes to choose and `K` samples from each class. So total batch size is `P*K`.\n",
    "6. Crop initial images - If crop augmentation is used, we would need to supply initial crop width and height.\n",
    "7. Network input size - Imagenet is generally trained with `224x224` and apparels are generally isotropic in dimensions, so we should stick to that. (Notice for training for person re-id, this assumption is not generally applicable as a person's `height > width`).\n",
    "8. Mining variant: BS, BH, BA\n",
    "9. Learning rate: Generally a low setting if we're utilizing pre-trained network.\n",
    "10. Learning rate decay: Number of iterations before dropping the learning rate. \n",
    "11. Metric to compare: Choices are `square_euclidean` or `euclidean`. In practicle `euclidean` seems to work better (also on this dataset).\n",
    "12. Margin: We will use the `softplus` option. Other possiblity would be to use `hard margin`, by supplying a float parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration and Download Imagenet pre-trained weights\n",
    "The following bash script could be directly use for training on fashion dataset.\n",
    "\n",
    "*The paths should be set appropriately* - Train csv file, Output location, Input pre--trained model, Image file location\n",
    "\n",
    "### Download the pre-trained Mobilenet-v1 from [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) and put inside `/datasets/train/pre-trained/`. Alternatively you could choose any available architecture from Resnet-50, Resnet-101. \n",
    "Please make sure to unzip the tar.gz file - `tar -xvzf mobilenet_v1_1.0_224.tgz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/sh\n",
    "####\n",
    "#### This file calls train.py with all hyperparameters as for the triplet metric learning experiment on In--Store Shopping Retrieval Project.\n",
    "\n",
    "source ~/venv/tf_1.9.0_cuda9.2/bin/activate ## This is needed if you're using virtual environment in python.\n",
    "\n",
    "cd /code/triplet-reid ## IMP: Change to the clone directory as to run the code\n",
    "\n",
    "IMAGE_ROOT=/datasets/fashion/\n",
    "EXP_ROOT=/datasets/train/BS_fashion/ ## THIS WILL BE THE OUTPUT FOLDER\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "    --train_set /datasets/fashion/in_shop_defense_triplet_loss_format_TRAIN.csv \\\n",
    "    --model_name mobilenet_v1_1_224 \\\n",
    "    --image_root $IMAGE_ROOT \\\n",
    "    --initial_checkpoint /datasets/train/pre-trained/mobilenet_v1_1.0_224.ckpt\\\n",
    "    --experiment_root $EXP_ROOT \\\n",
    "    --flip_augment \\\n",
    "    --embedding_dim 128 \\\n",
    "    --batch_p 18 \\\n",
    "    --batch_k 4 \\\n",
    "    --net_input_height 224 --net_input_width 224 \\\n",
    "    --margin soft \\\n",
    "    --metric euclidean \\\n",
    "    --loss batch_all \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --train_iterations 100000 \\\n",
    "    --head_name direct \\\n",
    "    --decay_start_iteration 25000\\\n",
    "    \"$@\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the trainig progress:\n",
    "\n",
    "### Training logs will be stored in the output folder above - i.e. `datasets/train/BS_fashion/`\n",
    "Users could use `tail -f /datasets/train/BS_fashion/train.log` to see ETA and related details on the terminal.\n",
    "\n",
    "### Visualizing the training progress\n",
    "Tensorflow training can be monitored with Tensorbord which provides api to visualize training progress, e.g. train precision at current epoch and more. More details could be found here `https://www.tensorflow.org/tensorboard`\n",
    "\n",
    "\n",
    "* In order to run tensorboard from terminal - `tensorboard logdir=/datasets/train/BS_fashion` \n",
    "* This will launch a visualizer on a browser - `localhost:6006`. The exact IP address could also be seen on terminal echo area.\n",
    "\n",
    "#### Sample healthy run\n",
    "\n",
    "\n",
    "As demonstrated by the authors of above could, we could use `tensorboard` to visualize the output. \n",
    "Here is an output of healthy run - healthy-run-sample\n",
    "\n",
    "\n",
    "![title](healthy-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a trained model, Quantitatively\n",
    "\n",
    "1. Step 1: Generate embeddings (stored in `.h5`) file using `embed.py` for both `Query` and `Test` set.\n",
    "\n",
    "*Note* By default the `.h5` files are stored in the `training-output` directory.\n",
    "\n",
    "2. Step 2: Evaluate these embeddings using `evaluate.py`. This will geneerate the `top-k` and `mAP` on the terminal.\n",
    "\n",
    "Lets look at both these steps in the cells below.\n",
    "\n",
    "A sample output from `evaluate.py` would look like: \n",
    "`mAP: 72.40% | top-1: 86.40% top-2: 91.22% | top-5: 95.43% | top-10:96.85% | top-20: 97.83%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading args from /datasets/train/BS_fashion/args.json.\n",
      "Evaluating using the following parameters:\n",
      "aggregator: None\n",
      "batch_k: 4\n",
      "batch_p: 18\n",
      "batch_size: 256\n",
      "checkpoint: checkpoint-100000\n",
      "checkpoint_frequency: 1000\n",
      "crop_augment: None\n",
      "dataset: /datasets/fashion/in_shop_defense_triplet_loss_format_QUERY.csv\n",
      "decay_start_iteration: 25000\n",
      "detailed_logs: False\n",
      "embedding_dim: 128\n",
      "experiment_root: /datasets/train/BS_fashion/\n",
      "filename: /datasets/train/BS_fashion/in_shop_defense_triplet_loss_format_QUERY_embeddings.h5\n",
      "flip_augment: False\n",
      "head_name: direct\n",
      "image_root: /datasets/fashion/\n",
      "initial_checkpoint: /datasets/train/pre-trained/mobilenet_v1_1.0_224.ckpt\n",
      "learning_rate: 0.0003\n",
      "loading_threads: 8\n",
      "loss: batch_all\n",
      "loss_ignore_zero: False\n",
      "margin: soft\n",
      "metric: euclidean\n",
      "model_name: mobilenet_v1_1_224\n",
      "net_input_height: 224\n",
      "net_input_width: 224\n",
      "optim: AdamOptimizer(learning_rate)\n",
      "pre_crop_height: 288\n",
      "pre_crop_width: 144\n",
      "quiet: False\n",
      "resume: False\n",
      "train_iterations: 100000\n",
      "train_set: /datasets/fashion/in_shop_defense_triplet_loss_format_TRAIN.csv\n",
      "Restoring from checkpoint: /datasets/train/BS_fashion/checkpoint-100000\n",
      "\r",
      "Embedded batch 0-256/14218\r",
      "Embedded batch 256-512/14218\r",
      "Embedded batch 512-768/14218\r",
      "Embedded batch 768-1024/14218\r",
      "Embedded batch 1024-1280/14218\r",
      "Embedded batch 1280-1536/14218\r",
      "Embedded batch 1536-1792/14218\r",
      "Embedded batch 1792-2048/14218\r",
      "Embedded batch 2048-2304/14218\r",
      "Embedded batch 2304-2560/14218\r",
      "Embedded batch 2560-2816/14218\r",
      "Embedded batch 2816-3072/14218\r",
      "Embedded batch 3072-3328/14218\r",
      "Embedded batch 3328-3584/14218\r",
      "Embedded batch 3584-3840/14218\r",
      "Embedded batch 3840-4096/14218\r",
      "Embedded batch 4096-4352/14218\r",
      "Embedded batch 4352-4608/14218\r",
      "Embedded batch 4608-4864/14218\r",
      "Embedded batch 4864-5120/14218\r",
      "Embedded batch 5120-5376/14218\r",
      "Embedded batch 5376-5632/14218\r",
      "Embedded batch 5632-5888/14218\r",
      "Embedded batch 5888-6144/14218\r",
      "Embedded batch 6144-6400/14218\r",
      "Embedded batch 6400-6656/14218\r",
      "Embedded batch 6656-6912/14218\r",
      "Embedded batch 6912-7168/14218\r",
      "Embedded batch 7168-7424/14218\r",
      "Embedded batch 7424-7680/14218\r",
      "Embedded batch 7680-7936/14218\r",
      "Embedded batch 7936-8192/14218\r",
      "Embedded batch 8192-8448/14218\r",
      "Embedded batch 8448-8704/14218\r",
      "Embedded batch 8704-8960/14218\r",
      "Embedded batch 8960-9216/14218\r",
      "Embedded batch 9216-9472/14218\r",
      "Embedded batch 9472-9728/14218\r",
      "Embedded batch 9728-9984/14218\r",
      "Embedded batch 9984-10240/14218\r",
      "Embedded batch 10240-10496/14218\r",
      "Embedded batch 10496-10752/14218\r",
      "Embedded batch 10752-11008/14218\r",
      "Embedded batch 11008-11264/14218\r",
      "Embedded batch 11264-11520/14218\r",
      "Embedded batch 11520-11776/14218\r",
      "Embedded batch 11776-12032/14218\r",
      "Embedded batch 12032-12288/14218\r",
      "Embedded batch 12288-12544/14218\r",
      "Embedded batch 12544-12800/14218\r",
      "Embedded batch 12800-13056/14218\r",
      "Embedded batch 13056-13312/14218\r",
      "Embedded batch 13312-13568/14218\r",
      "Embedded batch 13568-13824/14218\r",
      "Embedded batch 13824-14080/14218\r",
      "Embedded batch 14080-14218/14218\n",
      "Done with embedding, aggregating augmentations...\n",
      "Loading args from /datasets/train/BS_fashion/args.json.\n",
      "Evaluating using the following parameters:\n",
      "aggregator: None\n",
      "batch_k: 4\n",
      "batch_p: 18\n",
      "batch_size: 256\n",
      "checkpoint: checkpoint-100000\n",
      "checkpoint_frequency: 1000\n",
      "crop_augment: None\n",
      "dataset: /datasets/fashion/in_shop_defense_triplet_loss_format_GALLERY.csv\n",
      "decay_start_iteration: 25000\n",
      "detailed_logs: False\n",
      "embedding_dim: 128\n",
      "experiment_root: /datasets/train/BS_fashion/\n",
      "filename: /datasets/train/BS_fashion/in_shop_defense_triplet_loss_format_GALLERY_embeddings.h5\n",
      "flip_augment: False\n",
      "head_name: direct\n",
      "image_root: /datasets/fashion/\n",
      "initial_checkpoint: /datasets/train/pre-trained/mobilenet_v1_1.0_224.ckpt\n",
      "learning_rate: 0.0003\n",
      "loading_threads: 8\n",
      "loss: batch_all\n",
      "loss_ignore_zero: False\n",
      "margin: soft\n",
      "metric: euclidean\n",
      "model_name: mobilenet_v1_1_224\n",
      "net_input_height: 224\n",
      "net_input_width: 224\n",
      "optim: AdamOptimizer(learning_rate)\n",
      "pre_crop_height: 288\n",
      "pre_crop_width: 144\n",
      "quiet: False\n",
      "resume: False\n",
      "train_iterations: 100000\n",
      "train_set: /datasets/fashion/in_shop_defense_triplet_loss_format_TRAIN.csv\n",
      "Restoring from checkpoint: /datasets/train/BS_fashion/checkpoint-100000\n",
      "\r",
      "Embedded batch 0-256/12612\r",
      "Embedded batch 256-512/12612\r",
      "Embedded batch 512-768/12612\r",
      "Embedded batch 768-1024/12612\r",
      "Embedded batch 1024-1280/12612\r",
      "Embedded batch 1280-1536/12612\r",
      "Embedded batch 1536-1792/12612\r",
      "Embedded batch 1792-2048/12612\r",
      "Embedded batch 2048-2304/12612\r",
      "Embedded batch 2304-2560/12612\r",
      "Embedded batch 2560-2816/12612\r",
      "Embedded batch 2816-3072/12612\r",
      "Embedded batch 3072-3328/12612\r",
      "Embedded batch 3328-3584/12612\r",
      "Embedded batch 3584-3840/12612\r",
      "Embedded batch 3840-4096/12612\r",
      "Embedded batch 4096-4352/12612\r",
      "Embedded batch 4352-4608/12612\r",
      "Embedded batch 4608-4864/12612\r",
      "Embedded batch 4864-5120/12612\r",
      "Embedded batch 5120-5376/12612\r",
      "Embedded batch 5376-5632/12612\r",
      "Embedded batch 5632-5888/12612\r",
      "Embedded batch 5888-6144/12612\r",
      "Embedded batch 6144-6400/12612\r",
      "Embedded batch 6400-6656/12612\r",
      "Embedded batch 6656-6912/12612\r",
      "Embedded batch 6912-7168/12612\r",
      "Embedded batch 7168-7424/12612\r",
      "Embedded batch 7424-7680/12612\r",
      "Embedded batch 7680-7936/12612\r",
      "Embedded batch 7936-8192/12612\r",
      "Embedded batch 8192-8448/12612\r",
      "Embedded batch 8448-8704/12612\r",
      "Embedded batch 8704-8960/12612\r",
      "Embedded batch 8960-9216/12612\r",
      "Embedded batch 9216-9472/12612\r",
      "Embedded batch 9472-9728/12612\r",
      "Embedded batch 9728-9984/12612\r",
      "Embedded batch 9984-10240/12612\r",
      "Embedded batch 10240-10496/12612\r",
      "Embedded batch 10496-10752/12612\r",
      "Embedded batch 10752-11008/12612\r",
      "Embedded batch 11008-11264/12612\r",
      "Embedded batch 11264-11520/12612\r",
      "Embedded batch 11520-11776/12612\r",
      "Embedded batch 11776-12032/12612\r",
      "Embedded batch 12032-12288/12612\r",
      "Embedded batch 12288-12544/12612\r",
      "Embedded batch 12544-12612/12612\n",
      "Done with embedding, aggregating augmentations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /code/triplet-reid/nets/mobilenet_v1_1_224.py:17: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2020-02-16 07:48:06.553114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
      "name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:4c:00.0\n",
      "totalMemory: 10.73GiB freeMemory: 10.53GiB\n",
      "2020-02-16 07:48:06.697790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \n",
      "name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\n",
      "pciBusID: 0000:4b:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 9.92GiB\n",
      "2020-02-16 07:48:06.697824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2020-02-16 07:48:07.458774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-02-16 07:48:07.458804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2020-02-16 07:48:07.458809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N N \n",
      "2020-02-16 07:48:07.458812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   N N \n",
      "2020-02-16 07:48:07.459169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10174 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:4c:00.0, compute capability: 7.5)\n",
      "2020-02-16 07:48:07.570612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9600 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:4b:00.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:From /code/triplet-reid/nets/mobilenet_v1_1_224.py:17: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2020-02-16 07:48:21.379324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
      "name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:4c:00.0\n",
      "totalMemory: 10.73GiB freeMemory: 10.53GiB\n",
      "2020-02-16 07:48:21.495351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \n",
      "name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\n",
      "pciBusID: 0000:4b:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 9.92GiB\n",
      "2020-02-16 07:48:21.495384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2020-02-16 07:48:22.187127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-02-16 07:48:22.187154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2020-02-16 07:48:22.187159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N N \n",
      "2020-02-16 07:48:22.187162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   N N \n",
      "2020-02-16 07:48:22.187506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10174 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:4c:00.0, compute capability: 7.5)\n",
      "2020-02-16 07:48:22.298057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9593 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:4b:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Generate embeddings for the gallery and query images. Input to this code is the appropriate csv we generated above.\n",
    "\n",
    "### Set up bash paths to point to the code\n",
    "source ~/venv/tf_1.9.0_cuda9.2/bin/activate \n",
    "cd /code/triplet-reid\n",
    "## Generate embeddings for query set\n",
    "python embed.py --experiment_root /datasets/train/BS_fashion/ --dataset /datasets/fashion/in_shop_defense_triplet_loss_format_QUERY.csv --image_root /datasets/fashion/ --checkpoint checkpoint-100000\n",
    "\n",
    "## Generate embeddings for the gallery set\n",
    "python embed.py --experiment_root /datasets/train/BS_fashion/ --dataset /datasets/fashion/in_shop_defense_triplet_loss_format_GALLERY.csv --image_root /datasets/fashion/ --checkpoint checkpoint-100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating batch 0-256/14218\r",
      "Evaluating batch 256-512/14218\r",
      "Evaluating batch 512-768/14218\r",
      "Evaluating batch 768-1024/14218\r",
      "Evaluating batch 1024-1280/14218\r",
      "Evaluating batch 1280-1536/14218\r",
      "Evaluating batch 1536-1792/14218\r",
      "Evaluating batch 1792-2048/14218\r",
      "Evaluating batch 2048-2304/14218\r",
      "Evaluating batch 2304-2560/14218\r",
      "Evaluating batch 2560-2816/14218\r",
      "Evaluating batch 2816-3072/14218\r",
      "Evaluating batch 3072-3328/14218\r",
      "Evaluating batch 3328-3584/14218\r",
      "Evaluating batch 3584-3840/14218\r",
      "Evaluating batch 3840-4096/14218\r",
      "Evaluating batch 4096-4352/14218\r",
      "Evaluating batch 4352-4608/14218\r",
      "Evaluating batch 4608-4864/14218\r",
      "Evaluating batch 4864-5120/14218\r",
      "Evaluating batch 5120-5376/14218\r",
      "Evaluating batch 5376-5632/14218\r",
      "Evaluating batch 5632-5888/14218\r",
      "Evaluating batch 5888-6144/14218\r",
      "Evaluating batch 6144-6400/14218\r",
      "Evaluating batch 6400-6656/14218\r",
      "Evaluating batch 6656-6912/14218\r",
      "Evaluating batch 6912-7168/14218\r",
      "Evaluating batch 7168-7424/14218\r",
      "Evaluating batch 7424-7680/14218\r",
      "Evaluating batch 7680-7936/14218\r",
      "Evaluating batch 7936-8192/14218\r",
      "Evaluating batch 8192-8448/14218\r",
      "Evaluating batch 8448-8704/14218\r",
      "Evaluating batch 8704-8960/14218\r",
      "Evaluating batch 8960-9216/14218\r",
      "Evaluating batch 9216-9472/14218\r",
      "Evaluating batch 9472-9728/14218\r",
      "Evaluating batch 9728-9984/14218\r",
      "Evaluating batch 9984-10240/14218\r",
      "Evaluating batch 10240-10496/14218\r",
      "Evaluating batch 10496-10752/14218\r",
      "Evaluating batch 10752-11008/14218\r",
      "Evaluating batch 11008-11264/14218\r",
      "Evaluating batch 11264-11520/14218\r",
      "Evaluating batch 11520-11776/14218\r",
      "Evaluating batch 11776-12032/14218\r",
      "Evaluating batch 12032-12288/14218\r",
      "Evaluating batch 12288-12544/14218\r",
      "Evaluating batch 12544-12800/14218\r",
      "Evaluating batch 12800-13056/14218\r",
      "Evaluating batch 13056-13312/14218\r",
      "Evaluating batch 13312-13568/14218\r",
      "Evaluating batch 13568-13824/14218\r",
      "Evaluating batch 13824-14080/14218\r",
      "Evaluating batch 14080-14218/14218\n",
      "mAP: 69.37% | top-1: 84.01% top-2: 89.84% | top-5: 94.37% | top-10: 96.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-16 07:49:02.638374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
      "name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:4c:00.0\n",
      "totalMemory: 10.73GiB freeMemory: 10.53GiB\n",
      "2020-02-16 07:49:02.765705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \n",
      "name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\n",
      "pciBusID: 0000:4b:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 9.92GiB\n",
      "2020-02-16 07:49:02.765742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2020-02-16 07:49:03.509566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-02-16 07:49:03.509593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2020-02-16 07:49:03.509599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N N \n",
      "2020-02-16 07:49:03.509602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   N N \n",
      "2020-02-16 07:49:03.509990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10174 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:4c:00.0, compute capability: 7.5)\n",
      "2020-02-16 07:49:03.627740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9593 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:4b:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Quantitative testing - Using the embeddings generated from the cell above, we will compute accuracies\n",
    "### Set up bash paths to point to the code\n",
    "source ~/venv/tf_1.9.0_cuda9.2/bin/activate \n",
    "cd /code/triplet-reid\n",
    "\n",
    "python evaluate.py --excluder diagonal --query_dataset /datasets/fashion/in_shop_defense_triplet_loss_format_QUERY.csv --query_embeddings /datasets/train/BS_fashion/in_shop_defense_triplet_loss_format_QUERY_embeddings.h5 --gallery_dataset /datasets/fashion/in_shop_defense_triplet_loss_format_GALLERY.csv --gallery_embeddings /datasets/train/BS_fashion/in_shop_defense_triplet_loss_format_GALLERY_embeddings.h5 --metric euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a trained model, Qualitatively\n",
    "\n",
    "We could visualize the embeddings by showing them as following. First image in each row is the `query` image, while rest are `top-k` retrievals.\n",
    "\n",
    "![title](viz/sample_visuals/000184.png)\n",
    "![title](viz/sample_visuals/004032.png)\n",
    "![title](viz/sample_visuals/007947.png)\n",
    "\n",
    "##### Utility for above code\n",
    "* `python viz/viz_retrievals.py ---h` . \n",
    "* To use above script - we would need to point to query and gallery csv and .h5 embeddings (obtained from above quantitative testing)\n",
    "* `--output` is the desired output folder for above images.\n",
    "\n",
    "*Notice* that the above code uses Soptify's Annoy library (Approximate nearest neighbors) for efficient retrievals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get output qualitative images as shown in above cell.\n",
    "source ~/venv/tf_1.9.0_cuda9.2/bin/activate \n",
    "cd viz\n",
    "\n",
    "# Sample command: \n",
    "python viz_retrievals.py --img /datasets/fashion/ --query_csv /datasets/fashion/in_shop_defense_triplet_loss_format_QUERY.csv --query_h5 /datasets/train/BS_fashion/in_shop_defense_triplet_loss_format_QUERY_embeddings.h5 --gallery_csv /datasets/fashion/in_shop_defense_triplet_loss_format_GALLERY.csv --gallery_h5 /datasets/train/BS_fashion/in_shop_defense_triplet_loss_format_GALLERY_embeddings.h5 --k 5 --output top_5_viz_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting\n",
    "\n",
    "1. Every python code used above has `--h` arg option to list the required arguments and instructions.\n",
    "2. If you re-run \"train.py\" - make sure to look into `train.log` for an ETA or any mis-behaviors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning] *",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
